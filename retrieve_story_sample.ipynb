{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports libaries\n",
    "import requests\t\t\t\t\t\t# HTTP connection\n",
    "import time\t\t\t\t\t\t\t# timer\n",
    "import re \t\t\t\t\t\t\t# regular expression\n",
    "import csv\t\t\t\t\t\t\t# csv writer\n",
    "import pickle\t\t\t\t\t\t# exports lists\n",
    "import numpy as np\t\t\t\t\t# numerical computation\n",
    "from bs4 import BeautifulSoup\t\t# web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates random sample of users\n",
    "n = 5000\n",
    "max_id = 12600000\n",
    "storyids = np.random.randint(1, max_id, n)\n",
    "storyids = np.unique(storyids)\n",
    "storyids = [str(storyid) for storyid in storyids]\n",
    "baselink = 'https://www.fanfiction.net/s/'\n",
    "urls = [baselink + storyid for storyid in storyids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializes datasets\n",
    "data_stories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# collects data from user pages\n",
    "for i in range(0, len(storyids), 1):\n",
    "    \n",
    "    # retrieves user identifier\n",
    "    storyid = storyids[i]\n",
    "\n",
    "    # collects data from webpage\n",
    "    page = ''\n",
    "    while page == '':\n",
    "        try:\n",
    "            page = requests.get(urls[i])\n",
    "        except:\n",
    "            print(\"Connection refused, going to sleep...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "            \n",
    "    html = page.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # sets default for missing stories\n",
    "    userid = 'NA'\n",
    "    cat = 'NA'\n",
    "    title = 'NA'\n",
    "    summary = 'NA'\n",
    "    info = 'NA'\n",
    "    error = 'NA'\n",
    " \n",
    "    # collects story information if story exists\n",
    "    if soup.find('span', {'class': 'gui_warning'}) is None:\n",
    "        useridtag = soup.find('a', {'title': 'Send Private Message'})\n",
    "        cattag = soup.find('div', {'id': 'pre_story_links'})\n",
    "        titletag = soup.find('b', {'class': 'xcontrast_txt'})\n",
    "        summarytag = soup.find('div', {'class': 'xcontrast_txt', \n",
    "                                       'style' : 'margin-top:2px'})\n",
    "        infotag = soup.find('span', {'class': 'xgray xcontrast_txt'})\n",
    "        \n",
    "        if useridtag is None:\n",
    "            error = soup.find('span').text\n",
    "        else:\n",
    "            userid = useridtag['href']\n",
    "            cat = [link.text for link in cattag.find_all('a')]\n",
    "            title = titletag.text\n",
    "            summary = summarytag.text\n",
    "            info = infotag.text\n",
    "        \n",
    "    story = [storyid, userid, cat, title, summary, info, error]\n",
    "    data_stories.append(story)\n",
    "    \n",
    "time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exports data\n",
    "with open('data_stories', 'wb') as fp:\n",
    "    pickle.dump(data_stories, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
