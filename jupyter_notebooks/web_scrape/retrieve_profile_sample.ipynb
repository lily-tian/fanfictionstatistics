{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports libaries\n",
    "import requests\t\t\t\t\t\t# HTTP connection\n",
    "import random\t\t\t\t\t\t# random generator\n",
    "import time\t\t\t\t\t\t\t# timer\n",
    "import re \t\t\t\t\t\t\t# regular expression\n",
    "import pickle\t\t\t\t\t\t# exports lists\n",
    "import numpy as np\t\t\t\t\t# numerical computation\n",
    "from bs4 import BeautifulSoup\t\t# web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates random sample of users\n",
    "n = 10000\n",
    "max_id = 9000000\n",
    "userids = random.sample(range(1, max_id), n)\n",
    "baselink = 'https://www.fanfiction.net/u/'\n",
    "urls = [baselink + str(userid) for userid in userids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializes dataset\n",
    "data_profile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11408.562286615372\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# collects data from user pages\n",
    "for i in range(0, n, 1):\n",
    "    \n",
    "    # retrieves user identifier\n",
    "    userid = userids[i]\n",
    "\n",
    "    # collects data from webpage\n",
    "    page = ''\n",
    "    while page == '':\n",
    "        try:\n",
    "            page = requests.get(urls[i])\n",
    "        except:\n",
    "            print(\"Connection refused, going to sleep...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    html = requests.get(urls[i]).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # sets default for missing profile information\n",
    "    desc = 'NA'\n",
    "    country = 'NA'\n",
    "    join = 'NA'\n",
    "    profile = 'NA'\n",
    "    nlinks = 'NA'\n",
    "    \n",
    "    # collects profile information\n",
    "    desctag = soup.find('meta', {'name': 'description'})\n",
    "    countrytag = soup.find('img', {'height': '11', 'width': '16'})\n",
    "    jointag = soup.find(lambda tag: tag.name == 'span' and 'data-xutime' in tag.attrs)\n",
    "    profiletag = soup.find_all('p')\n",
    "    \n",
    "    if desctag is not None:\n",
    "        desc = desctag['content']\n",
    "    if countrytag is not None:\n",
    "        country = countrytag['title']\n",
    "    if jointag is not None:\n",
    "        join = jointag.text\n",
    "    if profiletag is not None:\n",
    "        profile = [tag.text for tag in soup.find_all('p')]\n",
    "        \n",
    "    nlinks_keys = [key['href'] for key in soup.find_all('a', {'data-toggle': 'tab'})]\n",
    "    nlinks_values = [value.text for value in soup.find_all('span', {'class': 'badge'})]        \n",
    "    nlinks = dict(zip(nlinks_keys, nlinks_values))\n",
    "    \n",
    "    profile = [userid, desc, country, join, profile, nlinks]\n",
    "    data_profile.append(profile)\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exports data\n",
    "with open('data_profile', 'wb') as fp:\n",
    "    pickle.dump(data_profile, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
